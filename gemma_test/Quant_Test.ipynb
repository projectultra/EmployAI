{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndDKzSkLbGJB",
        "outputId": "bfd86b01-e890-4681-a260-3e530b283107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AutoGPTQ' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PanQiWei/AutoGPTQ.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://about.benjaminmarie.com/data/py/auto-gptq-patch/_utils.py\n",
        "!wget https://about.benjaminmarie.com/data/py/auto-gptq-patch/auto.py\n",
        "\n",
        "!mv _utils.py AutoGPTQ/auto_gptq/modeling/\n",
        "!mv auto.py AutoGPTQ/auto_gptq/modeling/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYYtK0kBbMjt",
        "outputId": "6316b023-e1e1-4962-ca30-e0d80fd04c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-05 07:09:09--  https://about.benjaminmarie.com/data/py/auto-gptq-patch/_utils.py\n",
            "Resolving about.benjaminmarie.com (about.benjaminmarie.com)... 192.95.30.6\n",
            "Connecting to about.benjaminmarie.com (about.benjaminmarie.com)|192.95.30.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10839 (11K) [text/x-python]\n",
            "Saving to: ‘_utils.py’\n",
            "\n",
            "\r_utils.py             0%[                    ]       0  --.-KB/s               \r_utils.py           100%[===================>]  10.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-05 07:09:09 (90.9 MB/s) - ‘_utils.py’ saved [10839/10839]\n",
            "\n",
            "--2024-03-05 07:09:09--  https://about.benjaminmarie.com/data/py/auto-gptq-patch/auto.py\n",
            "Resolving about.benjaminmarie.com (about.benjaminmarie.com)... 192.95.30.6\n",
            "Connecting to about.benjaminmarie.com (about.benjaminmarie.com)|192.95.30.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4697 (4.6K) [text/x-python]\n",
            "Saving to: ‘auto.py’\n",
            "\n",
            "auto.py             100%[===================>]   4.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-05 07:09:09 (83.4 MB/s) - ‘auto.py’ saved [4697/4697]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd AutoGPTQ\n",
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkuLl_JzbRsR",
        "outputId": "4e21a9f0-dd2b-4f08-a0cd-0ea2ab316341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ\n",
            "Processing /content/AutoGPTQ\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (0.27.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (1.25.2)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (1.0.6)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (2.1.0+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (4.38.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq==0.8.0.dev0+cu1222) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq==0.8.0.dev0+cu1222) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq==0.8.0.dev0+cu1222) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq==0.8.0.dev0+cu1222) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq==0.8.0.dev0+cu1222) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq==0.8.0.dev0+cu1222) (3.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq==0.8.0.dev0+cu1222) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq==0.8.0.dev0+cu1222) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq==0.8.0.dev0+cu1222) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq==0.8.0.dev0+cu1222) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq==0.8.0.dev0+cu1222) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq==0.8.0.dev0+cu1222) (1.3.0)\n",
            "Building wheels for collected packages: auto-gptq\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for auto-gptq (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for auto-gptq\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for auto-gptq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "wlUX3Sp2bXvU",
        "outputId": "459b8198-c3df-43dd-bf8b-03c660da8aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'get_checkpoints' from 'auto_gptq.modeling._utils' (/content/AutoGPTQ/auto_gptq/modeling/_utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-32f239bf0f4e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mauto_gptq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/AutoGPTQ/auto_gptq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexllama_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexllama_set_max_input_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_gptq_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/AutoGPTQ/auto_gptq/modeling/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseGPTQForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPTQ_CAUSAL_LM_MODEL_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbaichuan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaiChuanGPTQForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbloom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBloomGPTQForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeGenGPTQForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/AutoGPTQ/auto_gptq/modeling/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_const\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCUDA_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSUPPORTED_MODELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from ._utils import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mautogptq_post_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mfind_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_checkpoints' from 'auto_gptq.modeling._utils' (/content/AutoGPTQ/auto_gptq/modeling/_utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace the following with your own Hugging Face access token.\n",
        "#This is my token (revocated, of course, never share your token online)\n",
        "access_token = \"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\"\n",
        "\n",
        "#The model we want to quantize\n",
        "pretrained_model_dir = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "#The name of the model once quantized\n",
        "#Note that we will only save the model, the tokenizer will remain the same\n",
        "quantized_model_dir = \"Llama-2-7b-4bit-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True, use_auth_token=access_token)\n",
        "#I copied and edited this function from AutoGPTQ repository\n",
        "def get_wikitext2(nsamples, seed, seqlen, model):\n",
        "\n",
        "    #I load the validation split since the training split is unecessary large\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n",
        "\n",
        "\n",
        "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(0)\n",
        "    torch.random.manual_seed(0)\n",
        "\n",
        "    traindataset = []\n",
        "\n",
        "    #This is unecessary to use the entire dataset for calibration\n",
        "    #Here I use only 128 samples\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        attention_mask = torch.ones_like(inp)\n",
        "        traindataset.append({'input_ids':inp,'attention_mask': attention_mask})\n",
        "    return traindataset"
      ],
      "metadata": {
        "id": "iW5wdN9wbraK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Process the data that will be used for calibration\n",
        "traindataset = get_wikitext2(128, 0, 2048, pretrained_model_dir)\n",
        "\n",
        "\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,  # quantize model to 4-bit\n",
        "    group_size=128,  # it is recommended to set the value to 128\n",
        "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
        ")\n",
        "\n",
        "\n",
        "# load Llama 2\n",
        "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config, use_auth_token=access_token)\n",
        "\n",
        "# quantize model, using traindataset for calibration\n",
        "#this may take up to 10 minutes on Google Colab Pro\n",
        "model.quantize(traindataset)\n",
        "\n",
        "# save quantized model using safetensors\n",
        "model.save_quantized(quantized_model_dir, use_safetensors=True)"
      ],
      "metadata": {
        "id": "VOlfVKKAdAZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We load the quantized model\n",
        "model = AutoGPTQForCausalLM.from_quantized(\"Llama-2-7b-4bit-chat-hf\", use_safetensors=True, device=\"cuda:0\", use_auth_token=False)\n",
        "\n",
        "#Your test prompt\n",
        "prompt = \"Tell me about gravity\"\n",
        "print(tokenizer.decode(model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(model.device), max_length=300)[0]))"
      ],
      "metadata": {
        "id": "XRRCE2hMdAvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUq4SfjzdBL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS IS USING A NEW THING CALLED OLLAMA FOR INFERENCE STUFF OF GEMMA 2B"
      ],
      "metadata": {
        "id": "ye4DW6PdiKqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O83EGFy8dBmA",
        "outputId": "6a46b190-8ab6-4e8f-f9a0-7e9b668246c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GREAT NOW I CANT EVEN RUN THAT ON THE TERMINAL BUT IT WORKS SOMEWHAT LIKE THIS\n",
        "\n",
        "[SEE THIS](https://www.youtube.com/watch?v=-MexTC18h20&list=RDCMUC-zVytOQB62OwMhKRi0TDvg&index=2)"
      ],
      "metadata": {
        "id": "O1OU5-6WjK5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4UlwxY5irSI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}