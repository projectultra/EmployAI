{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install redis\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"df.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import redis\n",
    "import requests\n",
    "from redis.commands.search.field import (\n",
    "    TextField,\n",
    "    VectorField,\n",
    ")\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = redis.Redis(\n",
    "  host='redis-18541.c305.ap-south-1-1.ec2.cloud.redislabs.com',\n",
    "  port=18541,\n",
    "  password='') #need to add github secrets\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  redis_key = f\"{row['Type']}:{index:03}\"\n",
    "  text = row['Question'] + \" \" + row['Answer']\n",
    "  data = {'Question': row['Question'],'Text': text}\n",
    "  json_text = json.dumps(data)\n",
    "  pipeline.json().set(redis_key, \"$\", json.loads(json_text))\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = client.json().get(\"Common:000\",\"$.Text\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = sorted(client.keys(\"Common:*\"))\n",
    "questions = client.json().mget(keys, \"$.Question\")\n",
    "questions = [item for sublist in questions for item in sublist]\n",
    "fulltext = client.json().mget(keys, \"$.Text\")\n",
    "fulltext = [item for sublist in fulltext for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('TinyLlama/TinyLlama-1.1B-Chat-v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = embedder.encode(questions).astype(np.float32).tolist()\n",
    "text_embeddings = embedder.encode(fulltext).astype(np.float32).tolist()\n",
    "VECTOR_DIMENSION = len(question_embeddings[0])\n",
    "VECTOR_DIMENSION #expected 2048 for tinyllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = client.pipeline()\n",
    "for key, question_embedding, text_embedding in zip(keys, question_embeddings,text_embeddings):\n",
    "    pipeline.json().set(key, \"$.question_embeddings\", question_embedding)\n",
    "    pipeline.json().set(key, \"$.text_embeddings\", text_embedding)\n",
    "pipeline.execute()\n",
    "client.json().get(\"Common:010\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = (\n",
    "    TextField(\"$.Question\", no_stem=True, as_name=\"Question\"),\n",
    "    TextField(\"$.Text\", no_stem=True, as_name=\"Text\"),\n",
    "    VectorField(\n",
    "        \"$.question_embeddings\",\n",
    "        \"FLAT\",\n",
    "        {\n",
    "            \"TYPE\": \"FLOAT32\",\n",
    "            \"DIM\": VECTOR_DIMENSION,\n",
    "            \"DISTANCE_METRIC\": \"COSINE\",\n",
    "        },\n",
    "        as_name=\"question_vector\",\n",
    "    ),\n",
    "    VectorField(\n",
    "        \"$.text_embeddings\",\n",
    "        \"FLAT\",\n",
    "        {\n",
    "            \"TYPE\": \"FLOAT32\",\n",
    "            \"DIM\": VECTOR_DIMENSION,\n",
    "            \"DISTANCE_METRIC\": \"COSINE\",\n",
    "        },\n",
    "        as_name=\"text_vector\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = IndexDefinition(prefix=[\"Common:\"], index_type=IndexType.JSON)\n",
    "res = client.ft(\"idx:double_vectors\").create_index(\n",
    "    fields=schema, definition=definition)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = client.ft(\"idx:double_vectors\").info()\n",
    "num_docs = info[\"num_docs\"]\n",
    "indexing_failures = info[\"hash_indexing_failures\"]\n",
    "print(f\"{num_docs} documents indexed with {indexing_failures} failures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    Query('(*)=>[KNN 5 @question_vector $query_vector AS vector_score]')\n",
    "     .sort_by('vector_score')\n",
    "     .return_fields('vector_score','Question','Text')\n",
    "     .dialect(2)\n",
    ")\n",
    "\n",
    "#change 5 to no of results and @for the the vector field name from schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_query = embedder.encode(\"Projects, Experience\")\n",
    "result = client.ft(\"idx:double_vectors\").search(query, { 'query_vector': np.array(encoded_query, dtype=np.float32).tobytes() })\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_table(query, queries, encoded_queries, extra_params={}):\n",
    "    results_list = []\n",
    "    for i, encoded_query in enumerate(encoded_queries):\n",
    "        result_docs = (\n",
    "            client.ft(\"idx:double_vectors\")\n",
    "            .search(\n",
    "                query,\n",
    "                {\n",
    "                    \"query_vector\": np.array(\n",
    "                        encoded_query, dtype=np.float32\n",
    "                    ).tobytes()\n",
    "                }\n",
    "                | extra_params,\n",
    "            )\n",
    "            .docs\n",
    "        )\n",
    "        for doc in result_docs:\n",
    "            vector_score = round(1 - float(doc.vector_score), 2)\n",
    "            results_list.append(\n",
    "                {\n",
    "                    \"query\": queries[i],\n",
    "                    \"Text\": doc.Text,\n",
    "                    \"score\": vector_score,\n",
    "                    \"id\": doc.id,\n",
    "                }\n",
    "            )\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "queries = [\n",
    "    \"Questions on projects, teamwork, collaboration, communication \",\n",
    "]\n",
    "encoded_queries = embedder.encode(queries)\n",
    "create_query_table(query, queries, encoded_queries)\n",
    "\n",
    "context = '''\n",
    "1 Question: How do you handle challenging situations or conflicts within a team? Answer: I approach challenging situations by (describe your approach, e.g., staying calm and seeking collaborative solutions). In a previous project at (previous company), I successfully resolved a conflict by (provide a brief example), emphasizing open communication and finding common ground.\n",
    "2 Question: How do you handle competing priorities and multiple projects simultaneously? Answer: I handle competing priorities by (mention your approach, e.g., setting clear priorities, multitasking efficiently). Utilizing time management skills and regularly reassessing priorities help me navigate and successfully manage multiple projects concurrently.\n",
    "3 Question: How do you handle situations where your team is not meeting its goals? Answer: Addressing a team not meeting its goals involves (mention your approach, e.g., identifying root causes, collaborating on solutions). I addressed a similar situation by (describe your actions, e.g., analyzing performance metrics, implementing targeted training).\n",
    "4 Question: How do you contribute to fostering a collaborative team environment? Answer: I contribute to a collaborative team environment by (mention your actions, e.g., promoting open communication, encouraging diverse perspectives). Building a positive and inclusive atmosphere is essential for achieving collective goals.\n",
    "5 Question: How do you foster a culture of continuous improvement within your team? Answer: Fostering a culture of continuous improvement involves (mention your strategies, e.g., encouraging feedback, promoting a growth mindset). I fostered such a culture in a previous team by (describe your specific actions, e.g., implementing regular retrospectives, celebrating small wins).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "role = None\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": '''You are an expert hiring manager and a highly skilled job trainer.\n",
    "        You have to provide the top interview questions and answers that the user may require for a job interview.\n",
    "        Use the questions and answers provided in the context and tailor them to the user.''',\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'''Context: {context}'''\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Give me 5 interview questions {role}with answers.\"\n",
    "    }\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
